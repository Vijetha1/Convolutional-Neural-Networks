<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Convolutional Neural Networks for Dummies by ragavvenkatesan</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-light.css">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Convolutional Neural Networks for Dummies</h1>
        <p>CNN implementaion in theano with many features such as dropouts, adagrad, momentum, svm layer and access to many datasets through skdata. </p>

        <p class="view"><a href="https://github.com/ragavvenkatesan/Convolutional-Neural-Networks">View the Project on GitHub <small>ragavvenkatesan/Convolutional-Neural-Networks</small></a></p>


        <ul>
          <li><a href="https://github.com/ragavvenkatesan/Convolutional-Neural-Networks/zipball/master">Download <strong>ZIP File</strong></a></li>
          <li><a href="https://github.com/ragavvenkatesan/Convolutional-Neural-Networks/tarball/master">Download <strong>TAR Ball</strong></a></li>
          <li><a href="https://github.com/ragavvenkatesan/Convolutional-Neural-Networks">View On <strong>GitHub</strong></a></li>
        </ul>
      </header>
      <section>
        <h1>
<a id="the-story-behind-the-repository" class="anchor" href="#the-story-behind-the-repository" aria-hidden="true"><span class="octicon octicon-link"></span></a>The story behind the repository</h1>

<p>This is a CNN implementaion in <a href="http://deeplearning.net/software/theano/">theano</a> with many modern day upgrades features such as dropouts[1], adagrad[2], momentum[3], max-margin layer and access to many datasets through <a href="https://jaberg.github.io/skdata/">skdata</a>. If you're a beginner in neural networks and deep learning as I was five months ago, you would find yourself overwhelmed with literature and materials. After spending a long time, based on your needs, you will end up deciding to use among many software, <a href="http://deeplearning.net/software/theano/">theano</a> . <a href="http://deeplearning.net/software/theano/">Theano</a> is a beautiful environment for it provides you gradients automatically. Its a symbolic language. IT IS NOT A DEEP LEARNING TOOLBOX. It is hard to get around, particularly, if like me you are also new to python and to symbolic programming in general. </p>

<p>You'll probably begin by following the <a href="http://nbviewer.ipython.org/github/craffel/theano-tutorial/blob/master/Theano%20Tutorial.ipynb">Caffe CNN tutorials</a> or <a href="http://www.iro.umontreal.ca/%7Elisa/pointeurs/tutorial_hpcs2011_fixed.pdf">Lisa lab's tutorial</a> or the <a href="http://deeplearning.net/software/theano/tutorial/index.html#tutorial">theano tutorial</a> itself.</p>

<p>Once you are done with the tutorials (and that takes about a couple of months to get everything going), while even though you may feel a little confident, you'll find that to stand up to the state-of-the-art, you need to implement SVM layers, All kinds of activation functions, dropout[1], classical momentum[3], accelerated gradients[4], adaptive gradient descent, AdaGrad[2] and all these before you can start your research.  </p>

<hr>

<h2>
<a id="what-is-in-this-repository" class="anchor" href="#what-is-in-this-repository" aria-hidden="true"><span class="octicon octicon-link"></span></a>What is in this repository?</h2>

<p>The code that is here in this repository has the following features, among many others:</p>

<ul>
<li>CNNs with easy architecture management. You can create any architecture you want with just changes of small parameters in the boiler plate. </li>
<li>Dropouts[1]</li>
<li>adaGrad[2]</li>
<li>Polyak Momentum[3]</li>
<li>Data handling capabilities.</li>
<li>Also provided a wrapper to <a href="https://jaberg.github.io/skdata/">skdata's dataset</a> interface (under construction, as of now, I have cifar10, mnist, extended mnists , caltech101). </li>
<li>[new] Data visualization capabilities: Visualize the activities of select images (random) from the trainset on each layer after select number of epochs of training. Also view filters after select number of epochs. If the input images are color, the first layer saves down color features. </li>
</ul>

<p>More features will be added as and when I am implementing them. You can check the <code>to_do.txt</code> in the repository for expected updates.  I will add more detailed description of the implementation as and when I have time to so. But don't expect it soon.</p>

<hr>

<h2>
<a id="system-requirements" class="anchor" href="#system-requirements" aria-hidden="true"><span class="octicon octicon-link"></span></a>System requirements</h2>

<p>Running this code essentially requires:</p>

<pre><code>1. python 2.x
2. theano 0.6 +
3. numpy 
4. scipy
5. skdata
6. cPickle
7. opencv (cv2)
8. gzip
</code></pre>

<p>Most of these could be installed by installing <a href="http://docs.continuum.io/anaconda/install.html">anaconda of continuum analytics</a> The code is reasonably well documented. It is not that difficult to find out from the boilerplate what is happening. If you need to understand really well what is happening before you jump into the code, use <code>verbose = True</code> flag in the boiler plate. The code has been tested on exhaustively in both MacOSX and Linux Ubuntu 14.x and 15.x by virtue of constant use. </p>

<hr>

<h2>
<a id="who-is-this-code-most-useful-for-" class="anchor" href="#who-is-this-code-most-useful-for-" aria-hidden="true"><span class="octicon octicon-link"></span></a>Who is this code most useful for ?</h2>

<p>I wrote this code essentially for my labmates, those who are interested in starting deep learning to make a fast transition into theano. I reckon that this will be useful for someone who is starting out to be a grad student getting to start research into deep learning (like me) or to someone who wants to do a try out on some kaggle challenge. Parts of the code are directly lifted from <a href="http://deeplearning.net/software/theano/tutorial/">theano tutorials</a> or from <a href="https://github.com/mdenil">Misha Denil's repository</a>. </p>

<p>This might not be really useful for advanced deep learning researchers. It's quite basic. If you are a serious researcher and you either find a bug or you want to just make suggestions please feel free, I will be grateful.  </p>

<hr>

<h2>
<a id="how-to-run-the-code-" class="anchor" href="#how-to-run-the-code-" aria-hidden="true"><span class="octicon octicon-link"></span></a>How to run the code ?</h2>

<p>I am not going to explain the details of the code explicitly. If you are in a position to want to change, probably this code is already too easy and obvious for you to understand and make these changes. If there are particular queries, raise an issue and I would love to respond. I would just explain the input parameters that help in running and also what to expect out of the output once ran. </p>

<h3>
<a id="optimization-related" class="anchor" href="#optimization-related" aria-hidden="true"><span class="octicon octicon-link"></span></a>Optimization Related:</h3>

<p>The first set of parameters is <code>optimization_params</code>. This contains packaged, the parameters that deal with optimiztion. Most parameters are easily recognizable in commentary. I will provide more details about others. </p>

<ul>
<li>
<code>mom_type</code> refers to the type of momentum being used. <code>mom_type = 0</code> implies no momentum and simple gradient descent. <code>mom_type = 1</code> runs Polyak Momentum [3], <code>mom_type = 2</code> runs Nestrov's accelareted gradient [4]. </li>
<li>
<code>mom_start</code> is the momentum that you would need at the begining of the implementation if you choose to use any momentum. <code>mom_end</code> is the momentum at the end after the <code>mom_interval</code> th epoch. Momentum runs increases linearly between epoch 0 and epoch <code>mom_interval</code> between <code>mom_start</code> and <code>mom_end</code>. Hinton recommends to start at 0.5 and end at 0.98. </li>
<li>
<code>l1_reg</code> and <code>l2_reg</code> are weight coefficients for L1 and L2 norms for all MLP and Regression layers.</li>
<li>
<code>ada_grad</code> is a flag that turns ON or OFF adagrad[2]. Similarly the <code>rms_prop</code> flag does the same for rms propagation[5].</li>
<li>
<code>fudge_factor</code> is a small number added to denominators to avoid division by <code>0</code>.</li>
</ul>

<h3>
<a id="files" class="anchor" href="#files" aria-hidden="true"><span class="octicon octicon-link"></span></a>Files:</h3>

<p>Files are defined in <code>filname_params</code>. </p>

<ul>
<li>
<code>results_file_name</code> save the results of classifcation along with predictions and probabilities on that file.</li>
<li>
<code>error_file_name</code> saves down the validation error after each epoch.</li>
<li>
<code>cost_file_name</code> saves the training cost of the network after each iteration. </li>
<li>
<code>confusion_file_name</code> saves down the confusion matrix. </li>
</ul>

<h3>
<a id="dataset-parameters" class="anchor" href="#dataset-parameters" aria-hidden="true"><span class="octicon octicon-link"></span></a>Dataset Parameters:</h3>

<p>Parameters related to datasets are defined in <code>data_params</code>. They are:</p>

<ul>
<li>
<code>type</code>: the type of data is to be used. The options are <code>pkl</code>, for loading theano tutorials type <code>pkl.gz</code> files; <code>skdata</code> for using the <a href="https://jaberg.github.io/skdata/">skdata</a> port, <code>mat</code> for loading custom datasets using Matlab's mat files. For loading <code>skdata</code> we have the following datasets available: <code>mnist</code>, <code>mnist_noise1</code> until <code>_noise6</code>, <code>mnist_bg_images</code>, <code>mnist_bg_rand</code>, <code>mnist_rotated</code>, <code>mnist_rotated_bg</code>, <code>cifar10</code>, <code>caltech101</code>. Use the <code>loc</code> variable to supply one of the options above for loading that particular <code>skdata</code>. Information on these datasets can be found in <a href="https://jaberg.github.io/skdata/">skdata</a> page. For the option <code>mat</code>, use loc to provide path to the main folder that contains the dataset. The folder should further contain subfolders <code>train</code>, <code>test</code> and <code>valid</code>, each of which should contain data in batches with the nomenclature <code>batch_1</code> upto <code>batch_n</code>. <code>n</code> in the <code>batch_n</code> variable is also to be provided in <code>batches2train</code>, <code>batches2test</code> and <code>batches2validate</code> for train, test and validate respectively. An example is provided in the git for how the data must be stored within each matfile.  Every epoch will go through <code>batches2train</code> number of mat files within each batch loading <code>batch_size</code> number of samples on the GPU ( or CPU depending on your theanorc config file ) to run each iteration of descent. </li>
<li>
<code>load_batches</code>: used to load data in batches from <code>skdata</code>, if not using, set it to <code>-1</code>. As of now used in <code>caltech101</code>. </li>
<li>
<code>height</code>, <code>width</code>, <code>channels</code>: refer to the image itself. An assertion error will be thrown if these conditions are not properly set.</li>
</ul>

<h3>
<a id="network-architecture" class="anchor" href="#network-architecture" aria-hidden="true"><span class="octicon octicon-link"></span></a>Network Architecture.</h3>

<p><code>arch_params</code> deals with parameters for the architecture of the network. Use this variable to arrive at a particular network architecture. </p>

<ul>
<li>
<code>n_epochs</code>: the training runs for these many epochs unless early termination conditions are reached.</li>
<li>
<code>validate_after_epochs</code>: one round of validation will be performed after these many epochs.</li>
<li>
<code>mlp_activations</code>: is an array. Provide one activation function per MLP layer. Options: <code>ReLU</code>, <code>Sigmoid</code> and <code>Tanh</code>.</li>
<li>
<code>cnn_activations</code>: is an array. Provide one activation function per CNN layer. Options are same as above.</li>
<li>
<code>dropout</code>  is a flag for dropout / backprop .Use along with <code>dropout_rates</code> an array for various probabilities of dropouts. One for each layer and one for input. The length of this array should be <code>number of MLP layers + 1</code>
</li>
<li>
<code>nkerns</code> is an array that defines the number of feature maps at each CNN layer</li>
<li>
<code>outs</code> is the number of output softmax nodes (or max-margin) required. Usually defined by number of unique labels to map network outputs to, or number of classes to classify.</li>
<li>
<code>filter_size</code> is an array that defines the size of the CNN receptive field. One for each CNN layer. </li>
<li>
<code>pooling_size</code> is an array that defines the CNN pooling size. One for each CNN layer.</li>
<li>
<code>num_nodes</code> is an array that defines the number of nodes in each MLP layer. This is also an array, one for each MLP layer.</li>
<li>
<code>svm_flag</code>: Flag that defines the last layer. <code>False</code> implies logistic regression, <code>True</code> implies max-margin hinge loss.<br>
</li>
</ul>

<h3>
<a id="visualization-parameters" class="anchor" href="#visualization-parameters" aria-hidden="true"><span class="octicon octicon-link"></span></a>Visualization parameters.</h3>

<ul>
<li>
<code>visualize_flag</code> set True makes the CNN save down filters, activations for random images.</li>
<li> <code>display_flag</code>  will make image be displayed.</li>
</ul>

<p>Using only these features one must be able to run some sophisticated network designs. </p>

<hr>

<h1>
<a id="references" class="anchor" href="#references" aria-hidden="true"><span class="octicon octicon-link"></span></a>References</h1>

<p>[1]   Srivastava, Nitish, et al. "Dropout: A simple way to prevent neural networks from overfitting." The Journal of Machine Learning Research 15.1 (2014): 1929-1958.</p>

<p>[2]   John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. JMLR</p>

<p>[3]   Polyak, Boris Teodorovich. "Some methods of speeding up the convergence of iteration methods." USSR Computational Mathematics and Mathematical Physics 4.5 (1964): 1-17. Implementation was adapted from Sutskever, Ilya, et al. "On the importance of initialization and momentum in deep learning." Proceedings of the 30th international conference on machine learning (ICML-13). 2013.</p>

<p>[4]   Nesterov, Yurii. "A method of solving a convex programming problem with convergence rate O (1/k2)."   Soviet Mathematics Doklady. Vol. 27. No. 2. 1983. Adapted from <a href="https://blogs.princeton.edu/imabandit/2013/04/01/acceleratedgradientdescent/">Sebastien Bubeck's</a> blog.</p>

<p>[5] Yann N. Dauphin, Harm de Vries, Junyoung Chung, Yoshua Bengio,"RMSProp and equilibrated adaptive learning rates for non-convex optimization", or arXiv:1502.04390v1</p>

<hr>

<p>Thanks for using the code, hope you had fun.
Ragav Venkatesan</p>
      </section>
      <footer>
        <p>This project is maintained by <a href="https://github.com/ragavvenkatesan">ragavvenkatesan</a></p>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
    
  </body>
</html>
